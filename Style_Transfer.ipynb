{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://msvocds.blob.core.windows.net/coco2014/train2014.zip\">Training Daters</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([ 123.68, 116.779, 103.939])\n",
    "\n",
    "def vgg_layers(img_input, input_shape):\n",
    "    # Block 1\n",
    "    img_input = Input(tensor=img_input, shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                            WEIGHTS_PATH_NO_TOP,\n",
    "                            cache_subdir='models',\n",
    "                            file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    f = h5py.File(weights_path)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layer_names:\n",
    "            g = f[layer.name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGG19(img_input, input_shape):\n",
    "    \"\"\"\n",
    "    VGG19, but can take input_tensor, and load weights on VGG layers only\n",
    "    \"\"\"\n",
    "    model = Model(img_input, vgg_layers(img_input, input_shape), name='vgg19')\n",
    "    model = load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    return x - MEAN_PIXEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "STYLE_LAYERS = ('block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1',\n",
    "                'block5_conv1')\n",
    "\n",
    "CONTENT_LAYER = 'block4_conv2'\n",
    "\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "# TODO: remove need for this\n",
    "BATCH_SIZE = 1\n",
    "BATCH_SHAPE = (BATCH_SIZE, 256, 256, 3)\n",
    "\n",
    "def tensor_size(x):\n",
    "    return np.nanprod(np.array(K.int_shape(x), dtype=np.float))\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "def get_vgg_features(input, layers, input_shape):\n",
    "    if len(K.int_shape(input)) == 3:\n",
    "        input = K.expand_dims(input, axis=0)\n",
    "    input = preprocess_input(input)\n",
    "    vgg = VGG19(input, input_shape)\n",
    "    outputs = [layer.output for layer in vgg.layers if layer.name in layers]\n",
    "    return outputs\n",
    "  \n",
    "\n",
    "def calculate_content_loss(content_image, reconstructed_image,\n",
    "                           content_weight, image_shape):\n",
    "    content_features = get_vgg_features(\n",
    "            content_image, CONTENT_LAYER, image_shape)[0]\n",
    "    reconstructed_content_features = get_vgg_features(\n",
    "            reconstructed_image, CONTENT_LAYER, image_shape)[0]\n",
    "   \n",
    "    content_size = tensor_size(content_features)\n",
    "    content_loss = content_weight * (2 * l2_loss(\n",
    "        reconstructed_content_features - content_features) / content_size)\n",
    "    \n",
    "    return content_loss\n",
    "    \n",
    "def calculate_style_loss(style_image, reconstructed_image,\n",
    "                         style_weight, style_image_shape, content_image_shape):\n",
    "     # Get outputs of style and content images at VGG layers\n",
    "    style_vgg_features = get_vgg_features(\n",
    "            style_image, STYLE_LAYERS, style_image_shape)\n",
    "    reconstructed_style_vgg_features = get_vgg_features(\n",
    "            reconstructed_image, STYLE_LAYERS, content_image_shape)\n",
    "    \n",
    "    # Calculate the style features and content features\n",
    "    # Style features are the gram matrices of the VGG feature maps\n",
    "    style_grams = []\n",
    "    style_rec_grams = []\n",
    "    for features in style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "        # shape in K.reshape needs to be np.array to convert Dimension to int\n",
    "        # (should be fixed in newer versions of Tensorflow)\n",
    "        features = K.reshape(features, np.array((1, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_grams.append(gram)\n",
    "        \n",
    "    for features in reconstructed_style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "        \n",
    "        # None checks\n",
    "        # TODO: Can someone else look over this - without this check compile fails throwing an error of Nonetype * Nonetype is invalid\n",
    "        # this stems from h * w which are set to None in the compile step.\n",
    "        if h == None or w == None:\n",
    "            continue\n",
    "        size = h * w\n",
    "        features = K.reshape(features, np.array((BATCH_SIZE, size, filters)))\n",
    "        \n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_rec_grams.append(gram)       \n",
    "        \n",
    "    # Style loss\n",
    "    style_losses = []\n",
    "    for style_gram, style_rec_gram in zip(style_grams, style_rec_grams):\n",
    "        style_gram_size = tensor_size(style_gram)\n",
    "        l2 = l2_loss(style_gram - style_rec_gram)\n",
    "        style_losses.append(2 * l2 / style_gram_size)\n",
    "    \n",
    "    style_loss = style_weight * reduce(tf.add, style_losses, 0) / BATCH_SIZE\n",
    "    \n",
    "    return style_loss\n",
    "    \n",
    "    \n",
    "def calculate_tv_loss(x, tv_weight):\n",
    "    tv_y_size = tensor_size(x[:,1:,:,:])\n",
    "    tv_x_size = tensor_size(x[:,:,1:,:])\n",
    "    y_tv = l2_loss(x[:,1:,:,:] - x[:,:BATCH_SHAPE[1]-1,:,:])\n",
    "    x_tv = l2_loss(x[:,:,1:,:] - x[:,:,:BATCH_SHAPE[2]-1,:])\n",
    "    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/BATCH_SIZE\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def create_loss_fn(style_image, content_weight, style_weight, tv_weight):\n",
    "    style_image = tf.convert_to_tensor(style_image)\n",
    "\n",
    "    def style_transfer_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true - content_image\n",
    "        y_pred - reconstructed image\n",
    "        \"\"\"\n",
    "        content_image = y_true\n",
    "        reconstructed_image = y_pred\n",
    "        \n",
    "        content_loss = calculate_content_loss(content_image,\n",
    "                reconstructed_image, content_weight, CONTENT_TRAINING_SIZE)\n",
    "        style_loss = calculate_style_loss(style_image,\n",
    "                reconstructed_image, style_weight, K.int_shape(style_image),\n",
    "                CONTENT_TRAINING_SIZE)\n",
    "        tv_loss = calculate_tv_loss(reconstructed_image, tv_weight)\n",
    "\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "        return loss\n",
    "\n",
    "    return style_transfer_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Conv2D, Conv2DTranspose,\n",
    "        BatchNormalization, Input, Lambda)\n",
    "from keras.models import Model\n",
    "import keras.layers\n",
    "\n",
    "def Conv2DBatchNorm(inputs, filters, kernel_size,\n",
    "                    strides=1, activation='relu'):\n",
    "    return BatchNormalization()(\n",
    "            Conv2D(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DTransposeBatchNorm(inputs, filters, kernel_size,\n",
    "                              strides=1, activation=None):\n",
    "    return BatchNormalization()(\n",
    "            Conv2DTranspose(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DResidualBlock(inputs):\n",
    "    tmp     = Conv2DBatchNorm(inputs, 128, 3)\n",
    "    tmp2    = Conv2DBatchNorm(tmp, 128, 3, activation=None)\n",
    "    return keras.layers.add([tmp, tmp2]) \n",
    "\n",
    "# TODO: instance norm, init weights?\n",
    "def TransformNet(inputs):\n",
    "    conv1   = Conv2DBatchNorm(inputs, 32, 9)\n",
    "    conv2   = Conv2DBatchNorm(conv1, 64, 3, strides=2)\n",
    "    conv3   = Conv2DBatchNorm(conv2, 128, 3, strides=2)\n",
    "    resid1  = Conv2DResidualBlock(conv3)\n",
    "    resid2  = Conv2DResidualBlock(resid1)\n",
    "    resid3  = Conv2DResidualBlock(resid2)\n",
    "    resid4  = Conv2DResidualBlock(resid3)\n",
    "    resid5  = Conv2DResidualBlock(resid4)\n",
    "    conv_t1 = Conv2DTransposeBatchNorm(conv3, 64, 3, strides=2)\n",
    "    conv_t2 = Conv2DTransposeBatchNorm(conv_t1, 32, 3, strides=2)\n",
    "    conv_t3 = Conv2DBatchNorm(conv_t2, 3, 9, activation='tanh')\n",
    "    preds = Lambda(lambda x : x * 150 + 255./2)(conv_t3)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 0 classes.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\", line 612, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"<ipython-input-55-d3b186b040f4>\", line 9, in tuple_gen\n",
      "    for img in gen:\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 727, in __next__\n",
      "    return self.next(*args, **kwargs)\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 950, in next\n",
      "    index_array, current_index, current_batch_size = next(self.index_generator)\n",
      "  File \"/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/preprocessing/image.py\", line 710, in _flow_index\n",
      "    current_index = (self.batch_index * batch_size) % n\n",
      "ZeroDivisionError: integer division or modulo by zero\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-50f6508d02bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train2014'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m82783\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1863\u001b[0m                                          \u001b[0;34m'a tuple `(x, y, sample_weight)` '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1864\u001b[0m                                          \u001b[0;34m'or `(x, y)`. Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1865\u001b[0;31m                                          str(generator_output))\n\u001b[0m\u001b[1;32m   1866\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: None"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "CONTENT_WEIGHT = 0.5\n",
    "STYLE_WEIGHT = 100\n",
    "TV_WEIGHT = 200\n",
    "\n",
    "style_img_path = 'wave.jpg'\n",
    "style_img = image.load_img(style_img_path)\n",
    "style_target = image.img_to_array(style_img)\n",
    "\n",
    "# Needed so that certain layers function in training mode (batch norm)\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "transform_net = TransformNet(inputs)\n",
    "model = Model(inputs=inputs, outputs=transform_net)\n",
    "\n",
    "loss_fn = create_loss_fn(style_target, CONTENT_WEIGHT, STYLE_WEIGHT, TV_WEIGHT)\n",
    "\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "gen = create_gen('data', target_size=(256, 256), batch_size=1)\n",
    "\n",
    "model.fit_generator(gen(), steps_per_epoch=82783)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
