{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://msvocds.blob.core.windows.net/coco2014/train2014.zip\">Training Daters</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([ 123.68, 116.779, 103.939])\n",
    "\n",
    "def vgg_layers(img_input, input_shape):\n",
    "    # Block 1\n",
    "    img_input = Input(tensor=img_input, shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                            WEIGHTS_PATH_NO_TOP,\n",
    "                            cache_subdir='models',\n",
    "                            file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    f = h5py.File(weights_path)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layer_names:\n",
    "            g = f[layer.name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGG19(img_input, input_shape):\n",
    "    \"\"\"\n",
    "    VGG19, but can take input_tensor, and load weights on VGG layers only\n",
    "    \"\"\"\n",
    "    model = Model(img_input, vgg_layers(img_input, input_shape), name='vgg19')\n",
    "    model = load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    return x - MEAN_PIXEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vgg import VGG19, preprocess_input\n",
    "\n",
    "STYLE_LAYERS = ('block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1',\n",
    "                'block5_conv1')\n",
    "\n",
    "CONTENT_LAYER = 'block4_conv2'\n",
    "\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "# TODO: remove need for this\n",
    "BATCH_SIZE = 1\n",
    "BATCH_SHAPE = (BATCH_SIZE, 256, 256, 3)\n",
    "\n",
    "def tensor_size(x):\n",
    "    return np.nanprod(np.array(K.int_shape(x), dtype=np.float))\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "def get_vgg_features(input, layers, input_shape):\n",
    "    if len(K.int_shape(input)) == 3:\n",
    "        input = K.expand_dims(input, axis=0)\n",
    "    input = preprocess_input(input)\n",
    "    vgg = VGG19(input, input_shape)\n",
    "    outputs = [layer.output for layer in vgg.layers if layer.name in layers]\n",
    "    return outputs\n",
    "  \n",
    "\n",
    "def calculate_content_loss(content_image, reconstructed_image,\n",
    "                           content_weight, image_shape):\n",
    "    content_features = get_vgg_features(\n",
    "            content_image, CONTENT_LAYER, image_shape)[0]\n",
    "    reconstructed_content_features = get_vgg_features(\n",
    "            reconstructed_image, CONTENT_LAYER, image_shape)[0]\n",
    "   \n",
    "    content_size = tensor_size(content_features)\n",
    "    content_loss = content_weight * (2 * l2_loss(\n",
    "        reconstructed_content_features - content_features) / content_size)\n",
    "    \n",
    "    return content_loss\n",
    "    \n",
    "def calculate_style_loss(style_image, reconstructed_image,\n",
    "                         style_weight, style_image_shape, content_image_shape):\n",
    "     # Get outputs of style and content images at VGG layers\n",
    "    style_vgg_features = get_vgg_features(\n",
    "            style_image, STYLE_LAYERS, style_image_shape)\n",
    "    reconstructed_style_vgg_features = get_vgg_features(\n",
    "            reconstructed_image, STYLE_LAYERS, content_image_shape)\n",
    "    \n",
    "    # Calculate the style features and content features\n",
    "    # Style features are the gram matrices of the VGG feature maps\n",
    "    style_grams = []\n",
    "    style_rec_grams = []\n",
    "    for features in style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        # shape in K.reshape needs to be np.array to convert Dimension to int\n",
    "        # (should be fixed in newer versions of Tensorflow)\n",
    "        features = K.reshape(features, np.array((1, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_grams.append(gram)\n",
    "        \n",
    "    for features in reconstructed_style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        # Need to know batch_size ahead of time\n",
    "        features = K.reshape(features, np.array((BATCH_SIZE, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_rec_grams.append(gram)       \n",
    "        \n",
    "    # Style loss\n",
    "    style_losses = []\n",
    "    for style_gram, style_rec_gram in zip(style_grams, style_rec_grams):\n",
    "        style_gram_size = tensor_size(style_gram)\n",
    "        l2 = l2_loss(style_gram - style_rec_gram)\n",
    "        style_losses.append(2 * l2 / style_gram_size)\n",
    "    \n",
    "    style_loss = style_weight * reduce(tf.add, style_losses) / BATCH_SIZE\n",
    "    \n",
    "    return style_loss\n",
    "    \n",
    "    \n",
    "def calculate_tv_loss(x, tv_weight):\n",
    "    tv_y_size = tensor_size(x[:,1:,:,:])\n",
    "    tv_x_size = tensor_size(x[:,:,1:,:])\n",
    "    y_tv = l2_loss(x[:,1:,:,:] - x[:,:BATCH_SHAPE[1]-1,:,:])\n",
    "    x_tv = l2_loss(x[:,:,1:,:] - x[:,:,:BATCH_SHAPE[2]-1,:])\n",
    "    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/BATCH_SIZE\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def create_loss_fn(style_image, content_weight, style_weight, tv_weight):\n",
    "    style_image = tf.convert_to_tensor(style_image)\n",
    "\n",
    "    def style_transfer_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true - content_image\n",
    "        y_pred - reconstructed image\n",
    "        \"\"\"\n",
    "\n",
    "        content_image = y_true\n",
    "        reconstructed_image = y_pred\n",
    "        \n",
    "        content_loss = calculate_content_loss(content_image,\n",
    "                reconstructed_image, content_weight, CONTENT_TRAINING_SIZE)\n",
    "        style_loss = calculate_style_loss(style_image,\n",
    "                reconstructed_image, style_weight, K.int_shape(style_image),\n",
    "                CONTENT_TRAINING_SIZE)\n",
    "        tv_loss = calculate_tv_loss(reconstructed_image, tv_weight)\n",
    "\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "        return loss\n",
    "\n",
    "    return style_transfer_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Conv2D, Conv2DTranspose,\n",
    "        BatchNormalization, Input, Lambda)\n",
    "from keras.models import Model\n",
    "import keras.layers\n",
    "\n",
    "def Conv2DBatchNorm(inputs, filters, kernel_size,\n",
    "                    strides=1, activation='relu'):\n",
    "    return BatchNormalization()(\n",
    "            Conv2D(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DTransposeBatchNorm(inputs, filters, kernel_size,\n",
    "                              strides=1, activation=None):\n",
    "    return BatchNormalization()(\n",
    "            Conv2DTranspose(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DResidualBlock(inputs):\n",
    "    tmp     = Conv2DBatchNorm(inputs, 128, 3)\n",
    "    tmp2    = Conv2DBatchNorm(tmp, 128, 3, activation=None)\n",
    "    return keras.layers.add([tmp, tmp2]) \n",
    "\n",
    "# TODO: instance norm, init weights?\n",
    "def TransformNet(inputs):\n",
    "    conv1   = Conv2DBatchNorm(inputs, 32, 9)\n",
    "    conv2   = Conv2DBatchNorm(conv1, 64, 3, strides=2)\n",
    "    conv3   = Conv2DBatchNorm(conv2, 128, 3, strides=2)\n",
    "    resid1  = Conv2DResidualBlock(conv3)\n",
    "    resid2  = Conv2DResidualBlock(resid1)\n",
    "    resid3  = Conv2DResidualBlock(resid2)\n",
    "    resid4  = Conv2DResidualBlock(resid3)\n",
    "    resid5  = Conv2DResidualBlock(resid4)\n",
    "    conv_t1 = Conv2DTransposeBatchNorm(conv3, 64, 3, strides=2)\n",
    "    conv_t2 = Conv2DTransposeBatchNorm(conv_t1, 32, 3, strides=2)\n",
    "    conv_t3 = Conv2DBatchNorm(conv_t2, 3, 9, activation='tanh')\n",
    "    preds = Lambda(lambda x : x * 150 + 255./2)(conv_t3)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'content.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6c9000ddff70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcontent_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'content.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mcontent_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_img_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# resize to 256x256 for training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size)\u001b[0m\n\u001b[1;32m    318\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    319\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2411\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content.jpg'"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "from transform import TransformNet\n",
    "from loss import create_loss_fn\n",
    "\n",
    "CONTENT_WEIGHT = 0.5\n",
    "STYLE_WEIGHT = 100\n",
    "TV_WEIGHT = 200\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen\n",
    "\n",
    "style_img_path = 'wave.jpg'\n",
    "style_img = image.load_img(style_img_path)\n",
    "style_target = image.img_to_array(style_img)\n",
    "\n",
    "content_img_path = 'content.jpg'\n",
    "content_img = image.load_img(content_img_path)\n",
    "\n",
    "# resize to 256x256 for training\n",
    "content_img = imresize(content_img, (256, 256, 3))\n",
    "content_target = image.img_to_array(content_img)\n",
    "\n",
    "# Needed so that certain layers function in training mode (batch norm)\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "transform_net = TransformNet(inputs)\n",
    "model = Model(inputs=inputs, outputs=transform_net)\n",
    "loss_fn = create_loss_fn(style_target, CONTENT_WEIGHT, STYLE_WEIGHT, TV_WEIGHT)\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "#content_target = np.expand_dims(content_target, axis=0)\n",
    "#X = [content_target]\n",
    "#y = [content_target]\n",
    "#model.fit(X, y)\n",
    "\n",
    "gen = create_gen('data', target_size=(256, 256), batch_size=1)\n",
    "model.fit_generator(gen, steps_per_epoch=82783)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
