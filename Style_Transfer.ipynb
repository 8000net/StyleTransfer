{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://msvocds.blob.core.windows.net/coco2014/train2014.zip\">Training Daters</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([ 123.68, 116.779, 103.939])\n",
    "\n",
    "def vgg_layers(img_input, input_shape):\n",
    "    # Block 1\n",
    "    img_input = Input(tensor=img_input, shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                            WEIGHTS_PATH_NO_TOP,\n",
    "                            cache_subdir='models',\n",
    "                            file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    f = h5py.File(weights_path)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layer_names:\n",
    "            g = f[layer.name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGG19(img_input, input_shape):\n",
    "    \"\"\"\n",
    "    VGG19, but can take input_tensor, and load weights on VGG layers only\n",
    "    \"\"\"\n",
    "    model = Model(img_input, vgg_layers(img_input, input_shape), name='vgg19')\n",
    "    model = load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    return x - MEAN_PIXEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from vgg import VGG19, preprocess_input\n",
    "\n",
    "STYLE_LAYERS = ('block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1',\n",
    "                'block5_conv1')\n",
    "\n",
    "CONTENT_LAYER = 'block4_conv2'\n",
    "\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "# TODO: remove need for this\n",
    "BATCH_SIZE = 1\n",
    "BATCH_SHAPE = (BATCH_SIZE, 256, 256, 3)\n",
    "\n",
    "def tensor_size(x):\n",
    "    return np.nanprod(np.array(K.int_shape(x), dtype=np.float))\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "def get_vgg_features(input, layers, input_shape):\n",
    "    if len(K.int_shape(input)) == 3:\n",
    "        input = K.expand_dims(input, axis=0)\n",
    "    input = preprocess_input(input)\n",
    "    vgg = VGG19(input, input_shape)\n",
    "    outputs = [layer.output for layer in vgg.layers if layer.name in layers]\n",
    "    return outputs\n",
    "  \n",
    "\n",
    "def calculate_content_loss(content_image, reconstructed_image,\n",
    "                           content_weight, image_shape):\n",
    "    content_features = get_vgg_features(\n",
    "            content_image, CONTENT_LAYER, image_shape)[0]\n",
    "    reconstructed_content_features = get_vgg_features(\n",
    "            reconstructed_image, CONTENT_LAYER, image_shape)[0]\n",
    "   \n",
    "    content_size = tensor_size(content_features)\n",
    "    content_loss = content_weight * (2 * l2_loss(\n",
    "        reconstructed_content_features - content_features) / content_size)\n",
    "    \n",
    "    return content_loss\n",
    "    \n",
    "def calculate_style_loss(style_image, reconstructed_image,\n",
    "                         style_weight, style_image_shape, content_image_shape):\n",
    "     # Get outputs of style and content images at VGG layers\n",
    "    style_vgg_features = get_vgg_features(\n",
    "            style_image, STYLE_LAYERS, style_image_shape)\n",
    "    reconstructed_style_vgg_features = get_vgg_features(\n",
    "            reconstructed_image, STYLE_LAYERS, content_image_shape)\n",
    "    \n",
    "    # Calculate the style features and content features\n",
    "    # Style features are the gram matrices of the VGG feature maps\n",
    "    style_grams = []\n",
    "    style_rec_grams = []\n",
    "    for features in style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        # shape in K.reshape needs to be np.array to convert Dimension to int\n",
    "        # (should be fixed in newer versions of Tensorflow)\n",
    "        features = K.reshape(features, np.array((1, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_grams.append(gram)\n",
    "        \n",
    "    for features in reconstructed_style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        # Need to know batch_size ahead of time\n",
    "        features = K.reshape(features, np.array((BATCH_SIZE, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_rec_grams.append(gram)       \n",
    "        \n",
    "    # Style loss\n",
    "    style_losses = []\n",
    "    for style_gram, style_rec_gram in zip(style_grams, style_rec_grams):\n",
    "        style_gram_size = tensor_size(style_gram)\n",
    "        l2 = l2_loss(style_gram - style_rec_gram)\n",
    "        style_losses.append(2 * l2 / style_gram_size)\n",
    "    \n",
    "    style_loss = style_weight * reduce(tf.add, style_losses) / BATCH_SIZE\n",
    "    \n",
    "    return style_loss\n",
    "    \n",
    "    \n",
    "def calculate_tv_loss(x, tv_weight):\n",
    "    tv_y_size = tensor_size(x[:,1:,:,:])\n",
    "    tv_x_size = tensor_size(x[:,:,1:,:])\n",
    "    y_tv = l2_loss(x[:,1:,:,:] - x[:,:BATCH_SHAPE[1]-1,:,:])\n",
    "    x_tv = l2_loss(x[:,:,1:,:] - x[:,:,:BATCH_SHAPE[2]-1,:])\n",
    "    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/BATCH_SIZE\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def create_loss_fn(style_image, content_weight, style_weight, tv_weight):\n",
    "    style_image = tf.convert_to_tensor(style_image)\n",
    "\n",
    "    def style_transfer_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true - content_image\n",
    "        y_pred - reconstructed image\n",
    "        \"\"\"\n",
    "\n",
    "        content_image = y_true\n",
    "        reconstructed_image = y_pred\n",
    "        \n",
    "        content_loss = calculate_content_loss(content_image,\n",
    "                reconstructed_image, content_weight, CONTENT_TRAINING_SIZE)\n",
    "        style_loss = calculate_style_loss(style_image,\n",
    "                reconstructed_image, style_weight, K.int_shape(style_image),\n",
    "                CONTENT_TRAINING_SIZE)\n",
    "        tv_loss = calculate_tv_loss(reconstructed_image, tv_weight)\n",
    "\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "        return loss\n",
    "\n",
    "    return style_transfer_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import (Conv2D, Conv2DTranspose,\n",
    "        BatchNormalization, Input, Lambda)\n",
    "from keras.models import Model\n",
    "import keras.layers\n",
    "\n",
    "def Conv2DBatchNorm(inputs, filters, kernel_size,\n",
    "                    strides=1, activation='relu'):\n",
    "    return BatchNormalization()(\n",
    "            Conv2D(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DTransposeBatchNorm(inputs, filters, kernel_size,\n",
    "                              strides=1, activation=None):\n",
    "    return BatchNormalization()(\n",
    "            Conv2DTranspose(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same'\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DResidualBlock(inputs):\n",
    "    tmp     = Conv2DBatchNorm(inputs, 128, 3)\n",
    "    tmp2    = Conv2DBatchNorm(tmp, 128, 3, activation=None)\n",
    "    return keras.layers.add([tmp, tmp2]) \n",
    "\n",
    "# TODO: instance norm, init weights?\n",
    "def TransformNet(inputs):\n",
    "    conv1   = Conv2DBatchNorm(inputs, 32, 9)\n",
    "    conv2   = Conv2DBatchNorm(conv1, 64, 3, strides=2)\n",
    "    conv3   = Conv2DBatchNorm(conv2, 128, 3, strides=2)\n",
    "    resid1  = Conv2DResidualBlock(conv3)\n",
    "    resid2  = Conv2DResidualBlock(resid1)\n",
    "    resid3  = Conv2DResidualBlock(resid2)\n",
    "    resid4  = Conv2DResidualBlock(resid3)\n",
    "    resid5  = Conv2DResidualBlock(resid4)\n",
    "    conv_t1 = Conv2DTransposeBatchNorm(conv3, 64, 3, strides=2)\n",
    "    conv_t2 = Conv2DTransposeBatchNorm(conv_t1, 32, 3, strides=2)\n",
    "    conv_t3 = Conv2DBatchNorm(conv_t2, 3, 9, activation='tanh')\n",
    "    preds = Lambda(lambda x : x * 150 + 255./2)(conv_t3)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/layers/core.py:640: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, 256, 256, 3)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Not a Keras tensor:', <tf.Tensor 'Const:0' shape=(884, 1280, 3) dtype=float32>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6c3982f60153>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_loss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONTENT_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTYLE_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTV_WEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m#content_target = np.expand_dims(content_target, axis=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mloss_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 910\u001b[0;31m                                         sample_weight, mask)\n\u001b[0m\u001b[1;32m    911\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \"\"\"\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/Desktop/StyleTransfer/loss.py\u001b[0m in \u001b[0;36mstyle_transfer_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 reconstructed_image, content_weight, CONTENT_TRAINING_SIZE)\n\u001b[1;32m    121\u001b[0m         style_loss = calculate_style_loss(style_image,\n\u001b[0;32m--> 122\u001b[0;31m                 \u001b[0mreconstructed_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m                 CONTENT_TRAINING_SIZE)\n\u001b[1;32m    124\u001b[0m         \u001b[0mtv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_tv_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstructed_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtv_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/lucaswood/anaconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36mint_shape\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not a Keras tensor:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Not a Keras tensor:', <tf.Tensor 'Const:0' shape=(884, 1280, 3) dtype=float32>)"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "\n",
    "from transform import TransformNet\n",
    "from loss import create_loss_fn\n",
    "\n",
    "CONTENT_WEIGHT = 0.5\n",
    "STYLE_WEIGHT = 100\n",
    "TV_WEIGHT = 200\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen\n",
    "\n",
    "style_img_path = 'wave.jpg'\n",
    "style_img = image.load_img(style_img_path)\n",
    "style_target = image.img_to_array(style_img)\n",
    "\n",
    "# Needed so that certain layers function in training mode (batch norm)\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "transform_net = TransformNet(inputs)\n",
    "model = Model(inputs=inputs, outputs=transform_net)\n",
    "loss_fn = create_loss_fn(style_target, CONTENT_WEIGHT, STYLE_WEIGHT, TV_WEIGHT)\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "gen = create_gen('data', target_size=(256, 256), batch_size=1)\n",
    "model.fit_generator(gen, steps_per_epoch=82783)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "content_img_path = 'content.jpg'\n",
    "content_img = image.load_img(content_img_path)\n",
    "\n",
    "# resize to 256x256 for training\n",
    "content_img = imresize(content_img, (256, 256, 3))\n",
    "content_target = image.img_to_array(content_img)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
