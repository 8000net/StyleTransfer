{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://msvocds.blob.core.windows.net/coco2014/train2014.zip\">Training Daters</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D, Input\n",
    "from keras.utils.data_utils import get_file\n",
    "import keras.backend as K\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "MEAN_PIXEL = np.array([ 123.68, 116.779, 103.939])\n",
    "\n",
    "def vgg_layers(img_input, input_shape):\n",
    "    # Block 1\n",
    "    img_input = Input(tensor=img_input, shape=input_shape)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4')(x)\n",
    "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def load_weights(model):\n",
    "    weights_path = get_file('vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "                            WEIGHTS_PATH_NO_TOP,\n",
    "                            cache_subdir='models',\n",
    "                            file_hash='253f8cb515780f3b799900260a226db6')\n",
    "    f = h5py.File(weights_path)\n",
    "    layer_names = [name for name in f.attrs['layer_names']]\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if layer.name in layer_names:\n",
    "            g = f[layer.name]\n",
    "            weights = [g[name] for name in g.attrs['weight_names']]\n",
    "            layer.set_weights(weights)\n",
    "            layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def VGG19(img_input, input_shape):\n",
    "    \"\"\"\n",
    "    VGG19, but can take input_tensor, and load weights on VGG layers only\n",
    "    \"\"\"\n",
    "    model = Model(img_input, vgg_layers(img_input, input_shape), name='vgg19')\n",
    "    model = load_weights(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def preprocess_input(x):\n",
    "    return x - MEAN_PIXEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "STYLE_LAYERS = ('block1_conv1', 'block2_conv1',\n",
    "                'block3_conv1', 'block4_conv1',\n",
    "                'block5_conv1')\n",
    "\n",
    "CONTENT_LAYERS = ('block4_conv2',)\n",
    "\n",
    "CONTENT_TRAINING_SIZE = (256, 256, 3)\n",
    "\n",
    "def tensor_size(x):\n",
    "    return np.nanprod(np.array(K.int_shape(x), dtype=np.float))\n",
    "\n",
    "def l2_loss(x):\n",
    "    return K.sum(K.square(x)) / 2\n",
    "\n",
    "def get_vgg_features(input, layers, input_shape):\n",
    "    if len(K.int_shape(input)) == 3:\n",
    "        input = K.expand_dims(input, axis=0)\n",
    "    input = preprocess_input(input)\n",
    "    vgg = VGG19(input, input_shape)\n",
    "    outputs = [layer.output for layer in vgg.layers if layer.name in layers]\n",
    "    return outputs\n",
    "  \n",
    "\n",
    "def calculate_content_loss(content_image, reconstructed_image,\n",
    "                           content_weight, image_shape, batch_size):\n",
    "    content_features = get_vgg_features(\n",
    "            content_image, CONTENT_LAYERS, image_shape)[0]\n",
    "    reconstructed_content_features = get_vgg_features(\n",
    "            reconstructed_image, CONTENT_LAYERS, image_shape)[0]\n",
    "   \n",
    "    content_size = tensor_size(content_features) * batch_size\n",
    "    content_loss = content_weight * (2 * l2_loss(\n",
    "        reconstructed_content_features - content_features) / content_size)\n",
    "    \n",
    "    return content_loss\n",
    "    \n",
    "def calculate_style_loss(style_image, reconstructed_image,\n",
    "                         style_weight, style_image_shape, content_image_shape,\n",
    "                         batch_size):\n",
    "     # Get outputs of style and content images at VGG layers\n",
    "    style_vgg_features = get_vgg_features(\n",
    "            style_image, STYLE_LAYERS, style_image_shape)\n",
    "    reconstructed_style_vgg_features = get_vgg_features(\n",
    "            reconstructed_image, STYLE_LAYERS, content_image_shape)\n",
    "    \n",
    "    # Calculate the style features of the style image and output image\n",
    "    # Style features are the gram matrices of the VGG feature maps\n",
    "    style_grams = []\n",
    "    style_rec_grams = []\n",
    "\n",
    "    # Style image style features\n",
    "    for features in style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        # shape in K.reshape needs to be np.array to convert Dimension to int\n",
    "        # (should be fixed in newer versions of Tensorflow)\n",
    "        features = K.reshape(features, np.array((1, h * w, filters)))\n",
    "\n",
    "        features_size = tensor_size(features)\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / features_size\n",
    "        style_grams.append(gram)\n",
    "        \n",
    "    # Output image style features\n",
    "    for features in reconstructed_style_vgg_features:\n",
    "        _, h, w, filters = K.int_shape(features)\n",
    "\n",
    "        size = h * w * filters\n",
    "        # Need to know batch_size ahead of time\n",
    "        features = K.reshape(features, np.array((batch_size, h * w, filters)))\n",
    "        features_T = tf.transpose(features, perm=[0,2,1])\n",
    "        gram = tf.matmul(features_T, features) / size\n",
    "        style_rec_grams.append(gram)       \n",
    "        \n",
    "    # Calculate style loss\n",
    "    style_losses = []\n",
    "    for style_gram, style_rec_gram in zip(style_grams, style_rec_grams):\n",
    "        style_gram_size = tensor_size(style_gram)\n",
    "        l2 = l2_loss(style_rec_gram - style_gram)\n",
    "        style_losses.append(2 * l2 / style_gram_size)\n",
    "    \n",
    "    style_loss = style_weight * reduce(tf.add, style_losses) / batch_size\n",
    "    \n",
    "    return style_loss\n",
    "    \n",
    "    \n",
    "def calculate_tv_loss(x, tv_weight, batch_size):\n",
    "    tv_y_size = tensor_size(x[:,1:,:,:])\n",
    "    tv_x_size = tensor_size(x[:,:,1:,:])\n",
    "    y_tv = l2_loss(x[:,1:,:,:] - x[:,:CONTENT_TRAINING_SIZE[0]-1,:,:])\n",
    "    x_tv = l2_loss(x[:,:,1:,:] - x[:,:,:CONTENT_TRAINING_SIZE[1]-1,:])\n",
    "    tv_loss = tv_weight*2*(x_tv/tv_x_size + y_tv/tv_y_size)/batch_size\n",
    "    return tv_loss\n",
    "\n",
    "\n",
    "def create_loss_fn(style_image, content_weight,\n",
    "                   style_weight, tv_weight, batch_size):\n",
    "    style_image = tf.convert_to_tensor(style_image)\n",
    "\n",
    "    def style_transfer_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true - content_image\n",
    "        y_pred - reconstructed image\n",
    "        \"\"\"\n",
    "\n",
    "        content_image = y_true\n",
    "        reconstructed_image = y_pred\n",
    "        \n",
    "        content_loss = calculate_content_loss(content_image,\n",
    "                                              reconstructed_image,\n",
    "                                              content_weight,\n",
    "                                              CONTENT_TRAINING_SIZE,\n",
    "                                              batch_size)\n",
    "        style_loss = calculate_style_loss(style_image,\n",
    "                                          reconstructed_image,\n",
    "                                          style_weight,\n",
    "                                          K.int_shape(style_image),\n",
    "                                          CONTENT_TRAINING_SIZE,\n",
    "                                          batch_size)\n",
    "        tv_loss = calculate_tv_loss(reconstructed_image, tv_weight, batch_size)\n",
    "\n",
    "        loss = content_loss + style_loss + tv_loss\n",
    "        return loss\n",
    "\n",
    "    return style_transfer_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Conv2DTranspose, Input, Lambda\n",
    "from keras.models import Model\n",
    "import keras.layers\n",
    "from keras.initializers import TruncatedNormal\n",
    "\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "\n",
    "WEIGHTS_INIT_STDEV = .1\n",
    "INIT = TruncatedNormal(stddev=WEIGHTS_INIT_STDEV, seed=1)\n",
    "\n",
    "def Conv2DInstanceNorm(inputs, filters, kernel_size,\n",
    "                    strides=1, activation='relu'):\n",
    "    return InstanceNormalization()(\n",
    "            Conv2D(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same',\n",
    "                kernel_initializer=INIT,\n",
    "                bias_initializer=INIT\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DTransposeInstanceNorm(inputs, filters, kernel_size,\n",
    "                              strides=1, activation=None):\n",
    "    return InstanceNormalization()(\n",
    "            Conv2DTranspose(\n",
    "                filters,\n",
    "                (kernel_size, kernel_size),\n",
    "                strides=strides,\n",
    "                activation=activation,\n",
    "                padding='same',\n",
    "                kernel_initializer=INIT,\n",
    "                bias_initializer=INIT\n",
    "            )(inputs))\n",
    "\n",
    "def Conv2DResidualBlock(inputs):\n",
    "    tmp     = Conv2DInstanceNorm(inputs, 128, 3)\n",
    "    tmp2    = Conv2DInstanceNorm(tmp, 128, 3, activation=None)\n",
    "    return keras.layers.add([tmp, tmp2]) \n",
    "\n",
    "def TransformNet(inputs):\n",
    "    conv1   = Conv2DInstanceNorm(inputs, 32, 9)\n",
    "    conv2   = Conv2DInstanceNorm(conv1, 64, 3, strides=2)\n",
    "    conv3   = Conv2DInstanceNorm(conv2, 128, 3, strides=2)\n",
    "    resid1  = Conv2DResidualBlock(conv3)\n",
    "    resid2  = Conv2DResidualBlock(resid1)\n",
    "    resid3  = Conv2DResidualBlock(resid2)\n",
    "    resid4  = Conv2DResidualBlock(resid3)\n",
    "    resid5  = Conv2DResidualBlock(resid4)\n",
    "    conv_t1 = Conv2DTransposeInstanceNorm(conv3, 64, 3, strides=2)\n",
    "    conv_t2 = Conv2DTransposeInstanceNorm(conv_t1, 32, 3, strides=2)\n",
    "    conv_t3 = Conv2DInstanceNorm(conv_t2, 3, 9, activation='tanh')\n",
    "    preds = Lambda(lambda x : x * 150 + 255./2)(conv_t3)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.misc import imresize\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CONTENT_WEIGHT = 0.5\n",
    "STYLE_WEIGHT = 100\n",
    "TV_WEIGHT = 200\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "def create_gen(img_dir, target_size, batch_size):\n",
    "    datagen = ImageDataGenerator()\n",
    "    gen = datagen.flow_from_directory(img_dir, target_size=target_size,\n",
    "                                      batch_size=batch_size, class_mode=None)\n",
    "\n",
    "    def tuple_gen():\n",
    "        for img in gen:\n",
    "            yield (img, img)\n",
    "\n",
    "    return tuple_gen()\n",
    "\n",
    "style_img_path = 'wave.jpg'\n",
    "style_img = image.load_img(style_img_path)\n",
    "style_target = image.img_to_array(style_img)\n",
    "\n",
    "content_img_path = 'content.jpg'\n",
    "content_img = image.load_img(content_img_path)\n",
    "\n",
    "# resize to 256x256 for training\n",
    "content_img = imresize(content_img, (256, 256, 3))\n",
    "content_target = image.img_to_array(content_img)\n",
    "\n",
    "# Needed so that certain layers function in training mode (batch norm)\n",
    "K.set_learning_phase(1)\n",
    "\n",
    "inputs = Input(shape=(256, 256, 3))\n",
    "transform_net = TransformNet(inputs)\n",
    "model = Model(inputs=inputs, outputs=transform_net)\n",
    "loss_fn = create_loss_fn(style_target, CONTENT_WEIGHT,\n",
    "                         STYLE_WEIGHT, TV_WEIGHT, BATCH_SIZE)\n",
    "model.compile(optimizer='adam', loss=loss_fn)\n",
    "\n",
    "#content_target = np.expand_dims(content_target, axis=0)\n",
    "#X = [content_target]\n",
    "#y = [content_target]\n",
    "#model.fit(X, y)\n",
    "\n",
    "gen = create_gen('data', target_size=(256, 256), batch_size=BATCH_SIZE)\n",
    "history = model.fit_generator(gen, steps_per_epoch=82780)\n",
    "model.save('wave-bs4.h5')\n",
    "pd.DataFrame(history.history).to_csv('wave-bs4.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "from scipy.misc import imresize, imsave\n",
    "\n",
    "from keras_contrib.layers import InstanceNormalization\n",
    "\n",
    "# Load model\n",
    "CONTENT_WEIGHT = 0.5\n",
    "STYLE_WEIGHT = 100\n",
    "TV_WEIGHT = 200\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "\n",
    "style_img_path = 'wave.jpg'\n",
    "style_img = image.load_img(style_img_path)\n",
    "style_target = image.img_to_array(style_img)\n",
    "\n",
    "loss_fn = create_loss_fn(style_target, CONTENT_WEIGHT,\n",
    "                         STYLE_WEIGHT, TV_WEIGHT, BATCH_SIZE)\n",
    "\n",
    "model = load_model('wave-bs4.h5', custom_objects={\n",
    "    'InstanceNormalization': InstanceNormalization,\n",
    "    'style_transfer_loss': loss_fn\n",
    "})\n",
    "\n",
    "\n",
    "# Get output\n",
    "content_img_path = 'content.jpg'\n",
    "content_img = image.load_img(content_img_path)\n",
    "\n",
    "content_img = imresize(content_img, (256, 256, 3))\n",
    "content_target = image.img_to_array(content_img)\n",
    "content_target = np.expand_dims(content_target, axis=0)\n",
    "\n",
    "\n",
    "output = model.predict([content_target])[0]\n",
    "imsave('doge-bs4.jpg', output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
